{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of IS-UserBased-Graph\n",
    "From the paper \"Graph-based context-aware collaborative filtering\" by Tu Minh Phuong, Do Thi Lien, Nguyen Duy Phuong\n",
    "\n",
    "Note: The dataset has been preprocessed by removing duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Imports\n",
    "\n",
    "import pandas as pd # Pandas for importing data\n",
    "from itertools import product # To calculate the product in item-splitting\n",
    "import numpy as np # General math stuff\n",
    "import networkx as nx # For generating graph networks\n",
    "from scipy.sparse import csr_matrix, lil_matrix, coo_matrix # for sparse matrix representation\n",
    "from collections import Counter # For summing up a list\n",
    "from sklearn.model_selection import KFold\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "full_data = pd.read_csv('../../data/ml100kC/context_ratings_sorted.csv', sep=';')\n",
    "\n",
    "kf = KFold(n_splits = 10, shuffle = True, random_state = 2)\n",
    "result = next(kf.split(full_data), None)\n",
    "\n",
    "train = full_data.iloc[result[0]]\n",
    "test =  full_data.iloc[result[1]]\n",
    "\n",
    "# Filter the test set. Used for the 20% withhold data they do in the paper.\n",
    "#y = test[['userId']]\n",
    "#test_precision_filter = test[y.replace(y.apply(pd.Series.value_counts)).gt(19).all(1)]\n",
    "\n",
    "# Calculate how much 20% of the data is for each user in test set\n",
    "#percentage20 = np.floor(test_precision_filter['userId'].value_counts()*0.20).astype(int)\n",
    "#test_userIds = test_precision_filter['userId'].unique()\n",
    "#withheld_data = [] \n",
    "\n",
    "#test_precision_filter = test_precision_filter.reset_index()\n",
    "\n",
    "#for userId in test_userIds:\n",
    " #   number_to_withhold = percentage20[userId]\n",
    "  #  for index, row in test_precision_filter.iterrows():\n",
    "   #     if row['userId'] == userId:\n",
    "    #       if number_to_withhold > 0:\n",
    "     #        withheld_data.append(row)\n",
    "      #       number_to_withhold -= 1\n",
    " \n",
    "#withheld_dataframe = pd.DataFrame(withheld_data)\n",
    "#test_precision_filter = test_precision_filter[~test_precision_filter.isin(withheld_dataframe)].dropna()\n",
    "       \n",
    "        \n",
    "\n",
    "\n",
    "#msk = np.random.rand(len(full_data)) < 0.80\n",
    "#train = full_data[msk] # Training data\n",
    "#test_data = full_data[~msk] # Test data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 (item splitting)\n",
    "Transform the original multi-dimensional matrix into 2D user x item rating matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 0: Generate all values for Nc1, Nc2 ... Ncn\n",
    "\n",
    "items_distinct = train.movieId.unique()\n",
    "\n",
    "# Context dimensions\n",
    "timeofday_distinct = train.timeofday.unique() # Included\n",
    "dayofweek_distinct = train.dayofweek.unique() # Included\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_dimensions_product = product(timeofday_distinct, dayofweek_distinct)\n",
    "T_temp = list(product(context_dimensions_product, items_distinct))\n",
    "              \n",
    "T = {k: v for k, v in enumerate(T_temp)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500 of 90752, time: 0.644 minutes, remaining: 22.750 minutes\n",
      "5000 of 90752, time: 1.272 minutes, remaining: 21.810 minutes\n",
      "7500 of 90752, time: 1.907 minutes, remaining: 21.168 minutes\n",
      "10000 of 90752, time: 2.642 minutes, remaining: 21.333 minutes\n",
      "12500 of 90752, time: 3.173 minutes, remaining: 19.862 minutes\n",
      "15000 of 90752, time: 3.885 minutes, remaining: 19.620 minutes\n",
      "17500 of 90752, time: 4.624 minutes, remaining: 19.356 minutes\n",
      "20000 of 90752, time: 5.330 minutes, remaining: 18.856 minutes\n",
      "22500 of 90752, time: 5.973 minutes, remaining: 18.120 minutes\n",
      "25000 of 90752, time: 6.598 minutes, remaining: 17.352 minutes\n",
      "27500 of 90752, time: 7.323 minutes, remaining: 16.844 minutes\n",
      "30000 of 90752, time: 7.984 minutes, remaining: 16.167 minutes\n",
      "32500 of 90752, time: 8.741 minutes, remaining: 15.667 minutes\n",
      "35000 of 90752, time: 9.328 minutes, remaining: 14.858 minutes\n",
      "37500 of 90752, time: 10.163 minutes, remaining: 14.433 minutes\n",
      "40000 of 90752, time: 10.907 minutes, remaining: 13.838 minutes\n",
      "42500 of 90752, time: 11.669 minutes, remaining: 13.248 minutes\n",
      "45000 of 90752, time: 12.492 minutes, remaining: 12.701 minutes\n",
      "47500 of 90752, time: 13.198 minutes, remaining: 12.018 minutes\n",
      "50000 of 90752, time: 13.984 minutes, remaining: 11.397 minutes\n",
      "52500 of 90752, time: 14.694 minutes, remaining: 10.706 minutes\n",
      "55000 of 90752, time: 15.302 minutes, remaining: 9.947 minutes\n",
      "57500 of 90752, time: 16.094 minutes, remaining: 9.307 minutes\n",
      "60000 of 90752, time: 16.874 minutes, remaining: 8.649 minutes\n",
      "62500 of 90752, time: 17.574 minutes, remaining: 7.944 minutes\n",
      "65000 of 90752, time: 18.247 minutes, remaining: 7.229 minutes\n",
      "67500 of 90752, time: 18.974 minutes, remaining: 6.536 minutes\n",
      "70000 of 90752, time: 19.813 minutes, remaining: 5.874 minutes\n",
      "72500 of 90752, time: 20.683 minutes, remaining: 5.207 minutes\n",
      "75000 of 90752, time: 21.496 minutes, remaining: 4.515 minutes\n",
      "77500 of 90752, time: 22.320 minutes, remaining: 3.817 minutes\n",
      "80000 of 90752, time: 23.008 minutes, remaining: 3.092 minutes\n",
      "82500 of 90752, time: 23.764 minutes, remaining: 2.377 minutes\n",
      "85000 of 90752, time: 24.483 minutes, remaining: 1.657 minutes\n",
      "87500 of 90752, time: 25.344 minutes, remaining: 0.942 minutes\n",
      "90000 of 90752, time: 26.018 minutes, remaining: 0.217 minutes\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Generate the two-dimensional matrix of ratings\n",
    "# This is saved as a sparse matrix, since a lot of the ratings will be unknown\n",
    "users_distinct = train.userId.unique()\n",
    "user_ids = {k: v for v, k in enumerate(users_distinct)}\n",
    "\n",
    "W = lil_matrix((len(users_distinct), len(T)), dtype=np.float32)\n",
    "\n",
    "# Iterate through ratings and put them in the matrix\n",
    "progress = 1\n",
    "start_time = time.time()\n",
    "train_size = len(train)\n",
    "for index, row in train.iterrows():\n",
    "    if progress % 2500 == 0:\n",
    "        time_spent = time.time() - start_time\n",
    "        time_per_iter = time_spent/progress\n",
    "        print(f'{progress} of {train_size}, time: {time_spent/60:.3f} minutes, remaining: {(((train_size - progress)*time_per_iter)/60):.3f} minutes')\n",
    "    \n",
    "    itemId = row['movieId']\n",
    "    userId = row['userId']\n",
    "    timeofday = row['timeofday']\n",
    "    dayofweek = row['dayofweek']\n",
    "    rating = row['rating']\n",
    "    \n",
    "    # Find the index for the item \n",
    "    idx = list(T.keys())[list(T.values()).index(((timeofday, dayofweek), itemId))]\n",
    "    \n",
    "    W[user_ids[userId], idx] = rating\n",
    "    progress += 1\n",
    "    \n",
    "W = csr_matrix(W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Graph construction\n",
    "In this step, we'll transform the users_by_T_matrix into a graph to allow us to do graph-based similarity in next step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#edgelist = []\n",
    "#users_in_graph = set([])\n",
    "\n",
    "#for user, item in zip(*users_by_T_matrix.nonzero()):\n",
    "#    rating_value = users_by_T_matrix[user, item]\n",
    "#    users_in_graph.add(\"u\" + str(user))\n",
    "#    edgelist.append(\"u\" + str(user) + \" i\" + str(item) + \" \" + str(rating_value/5))\n",
    "\n",
    "#graph = nx.bipartite.edgelist.parse_edgelist(edgelist, data=(('weight', float), ))\n",
    "#graph.remove_nodes_from(nx.isolates(graph))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Graph-based similarity calculation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate UZ and W \n",
    "# UZ = np.zeros((len(users_distinct), len(items_distinct)), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = 10 # Amount of jumps, must be an even number\n",
    "\n",
    "# Generate UZ_L based on the L value\n",
    "UZ2 = W.dot(np.transpose(W))\n",
    "UZL = UZ2\n",
    "# Use jumps to keep track of how many jumps we've made\n",
    "jumps = 2\n",
    "while jumps < L:\n",
    "    UZL = np.dot(UZ2, UZL)\n",
    "    jumps += 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This can also be done using the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement graph approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate recommendations\n",
    "Now that we have the most similar users, we can find the best items for a given user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(K1, K2, Ua, context, itemid=0):\n",
    "    \n",
    "    if Ua not in user_ids:\n",
    "        print(f'User {Ua} not in trainset: {user_ids}')\n",
    "        return [], []\n",
    "\n",
    "    fictional_id = user_ids[Ua]\n",
    "    \n",
    "    most_similar_users = []\n",
    "    for le, ri in zip(UZL.indptr[:-1], UZL.indptr[1:]):\n",
    "        most_similar_users = UZL.indices[le + np.argpartition(UZL.data[le:ri], -K1)[-K1:]]\n",
    "\n",
    "    # Find items that the user has rated yet\n",
    "    user_row = W[fictional_id]\n",
    "    \n",
    "    # Recommendations, they'll be the final ones.\n",
    "    recs = {}\n",
    "\n",
    "    # itemID, sum rating, rating count\n",
    "    # k: itemID, v: (sum rating, rating count)\n",
    "    rating_sum_counter = {}\n",
    "\n",
    "    for sim_user_row in most_similar_users:\n",
    "        for row, col in zip(*W[sim_user_row].nonzero()):\n",
    "            if W[fictional_id, col]:\n",
    "                continue\n",
    "\n",
    "            rating = W[sim_user_row, col]\n",
    "            if col not in rating_sum_counter:\n",
    "                rating_sum_counter[col] = (rating, 1)\n",
    "            else:\n",
    "                current_rating = rating_sum_counter[col]\n",
    "                new_rating = current_rating[0] + rating\n",
    "                new_count = current_rating[1] + 1\n",
    "                rating_sum_counter[col] = (new_rating, new_count)\n",
    "\n",
    "    # Go through dictionary and sum values\n",
    "    for k, v in rating_sum_counter.items():\n",
    "        rating_sum_counter[k] = v[0]/v[1]\n",
    "\n",
    "    # Sort list by highest values\n",
    "    counted_recs = Counter(rating_sum_counter)\n",
    "\n",
    "    # Map back from fictious item to actual item\n",
    "    item_ids = {v: k for v, k in enumerate(T)}\n",
    "\n",
    "    filtered_results = []\n",
    "    for item in counted_recs:\n",
    "        if T[item][0] == context:\n",
    "            filtered_results.append(T[item][1])\n",
    "\n",
    "    filtered_results = filtered_results[:K2]\n",
    "    \n",
    "    # Find predicted rating for item\n",
    "    ## Map from actual item with context to fictional item id\n",
    "    ## find rating for item if available\n",
    "    idx = -1\n",
    "    \n",
    "    predicted_rating = 0\n",
    "    for key, value in T.items():\n",
    "        if value == (context, itemid):\n",
    "            idx = key\n",
    "            if key in rating_sum_counter:\n",
    "                predicted_rating = rating_sum_counter[idx]\n",
    "                return filtered_results, predicted_rating\n",
    "            else:\n",
    "                break\n",
    "\n",
    "    return filtered_results, predicted_rating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate hit count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_precisions = []\n",
    "\n",
    "def calculate_map(topK_dict, k_val):\n",
    "    for key in topK_dict:\n",
    "        score = 0.0\n",
    "        number_of_hits = 0.0\n",
    "\n",
    "        for count, element in enumerate(topk_dict[key]):\n",
    "            if element in relevant_items:\n",
    "                number_of_hits += 1\n",
    "                score += number_of_hits / (i + 1)\n",
    "        \n",
    "        \n",
    "        if relevant_items.empty:\n",
    "            average_precisions.append(0.0)\n",
    "        elif len(relevant_items) < k_val:\n",
    "            average_precisions.append(score / len(relevant_items))\n",
    "        else:\n",
    "            average_precisions.append(score / k_val)\n",
    "    \n",
    "    return (1 / len(test_userIds) * sum(average_precisions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Neighbors | L | RMSE | K | MAP@K |\n",
      "|-----------|---|------|---|-------|\n",
      "554 of 5541, time: 6.978 minutes, remaining: 62.818 minutes\n",
      "1108 of 5541, time: 13.668 minutes, remaining: 54.685 minutes\n",
      "1662 of 5541, time: 20.236 minutes, remaining: 47.229 minutes\n",
      "2216 of 5541, time: 26.746 minutes, remaining: 40.132 minutes\n",
      "2770 of 5541, time: 33.336 minutes, remaining: 33.348 minutes\n",
      "3324 of 5541, time: 39.786 minutes, remaining: 26.536 minutes\n",
      "3878 of 5541, time: 46.088 minutes, remaining: 19.764 minutes\n",
      "4432 of 5541, time: 52.635 minutes, remaining: 13.171 minutes\n",
      "4986 of 5541, time: 58.839 minutes, remaining: 6.550 minutes\n",
      "5540 of 5541, time: 65.206 minutes, remaining: 0.012 minutes\n",
      "| 10 | 10 | 1.077 | 0.000 | 10\n"
     ]
    }
   ],
   "source": [
    "progress = 1\n",
    "test_set_size = len(test)\n",
    "neighbor_values = [10]\n",
    "K_values = [10]\n",
    "\n",
    "\n",
    "#relevant_items = [entry for entry in test_precision_filter if test_precision_filter['rating'] > 3]\n",
    "\n",
    "\n",
    "\n",
    "print('| Neighbors | L | RMSE | K | MAP@K |')\n",
    "print('|-----------|---|------|---|-------|')\n",
    "for k_val in K_values:\n",
    "    for neighbor_val in neighbor_values:\n",
    "        start_time = time.time()\n",
    "        neighbor_value = neighbor_val\n",
    "\n",
    "        actuals = []\n",
    "        predictions = []\n",
    "        rmse = 0\n",
    "        precision = 0\n",
    "        \n",
    "\n",
    "        for index, user in test.iterrows():          \n",
    "            if progress % (test_set_size // 10) == 0:\n",
    "                time_spent = time.time() - start_time\n",
    "                time_per_iter = time_spent/progress\n",
    "                print(f'{progress} of {test_set_size}, time: {time_spent/60:.3f} minutes, remaining: {(((test_set_size - progress)*time_per_iter)/60):.3f} minutes')\n",
    "            progress += 1\n",
    "\n",
    "            timeofday = user['timeofday']\n",
    "            dayofweek = user['dayofweek']\n",
    "\n",
    "            current_context = (timeofday, dayofweek)\n",
    "\n",
    "            item = user['movieId']\n",
    "            user_id = user['userId']\n",
    "            rating = user['rating']\n",
    "\n",
    "            topK, prediction = predict(K1=neighbor_value, K2=k_val, Ua=user_id, context=current_context, itemid=item)\n",
    "\n",
    "            if prediction != 0:\n",
    "                actuals.append(rating)\n",
    "                predictions.append(prediction)\n",
    "            \n",
    "        if len(predictions) > 0:\n",
    "            rmse = np.sqrt(np.mean((np.array(predictions)-np.array(actuals))**2))\n",
    "        \n",
    "\n",
    "        print(f'| {neighbor_value} | {L} | {rmse:.3f} | mapk | {k_val}')\n",
    "        progress = 1\n",
    "        start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "\n",
    "# U1 split\n",
    "| Neighbors | L  | RMSE   |\n",
    "|-----------|----|--------|\n",
    "| 20        | 10 | 1\\.168 |\n",
    "| 10        | 8  | 1\\.169 |\n",
    "| 20        | 6  | 1\\.172 |\n",
    "| 10        | 12 | 1\\.173 |\n",
    "| 10        | 10 | 1\\.173 |\n",
    "| 10        | 6  | 1\\.200 |\n",
    "| 5         | 8  | 1\\.216 |\n",
    "| 5         | 10 | 1\\.216 |\n",
    "| 5         | 6  | 1\\.231 |\n",
    "| 3         | 8  | 1\\.279 |\n",
    "| 3         | 10 | 1\\.279 |\n",
    "| 3         | 12 | 1\\.279 |\n",
    "| 20        | 4  | 1\\.294 |\n",
    "| 10        | 4  | 1\\.323 |\n",
    "| 3         | 6  | 1\\.337 |\n",
    "| 3         | 4  | 1\\.361 |\n",
    "| 5         | 4  | 1\\.365 |\n",
    "\n",
    "\n",
    "# Missing:\n",
    "U1: L4, N20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#How to get\n",
    "\n",
    "#relevant_items = withheld_dataframe.loc[(withheld_dataframe['rating'] > 3) & (withheld_dataframe['userId'] == key)]\n",
    "        \n",
    "#intersect_relevant_topk = relevant_items[relevant_items.index.isin(topK_dict[key])]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
